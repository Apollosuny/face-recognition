{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_embedding(face_image, model):\n",
    "    face_image = cv2.resize(face_image, (50, 50))\n",
    "    face_image = face_image.astype('float32')\n",
    "\n",
    "    mean, std = face_image.mean(), face_image.std()\n",
    "    face_image = (face_image - mean) / std\n",
    "\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "\n",
    "    embedding = model.predict(face_image)[0]\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data_and_extract_embeddings(train_data_dir, model):\n",
    "    faces = []\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for person_name in os.listdir(train_data_dir):\n",
    "        person_dir = os.path.join(train_data_dir, person_name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        \n",
    "        for filename in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            face_embedding = extract_face_embedding(image, model)\n",
    "\n",
    "            faces.append(image)\n",
    "            embeddings.append(face_embedding)\n",
    "            labels.append(person_name)\n",
    "    \n",
    "    return faces, embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 50, 50, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 50, 50, 3), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m faceNet_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20240606-model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load training data and extract embeddings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m faces, embeddings, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_train_data_and_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaceNet_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Chuyển đổi nhãn sang dạng số\u001b[39;00m\n\u001b[0;32m     11\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mload_train_data_and_extract_embeddings\u001b[1;34m(train_data_dir, model)\u001b[0m\n\u001b[0;32m     12\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(person_dir, filename)\n\u001b[0;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m---> 14\u001b[0m face_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m faces\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m     17\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(face_embedding)\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mextract_face_embedding\u001b[1;34m(face_image, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m face_image \u001b[38;5;241m=\u001b[39m (face_image \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[0;32m      8\u001b[0m face_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(face_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32md:\\Project\\face-recognition\\api\\.env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Project\\face-recognition\\api\\.env\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 50, 50, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 50, 50, 3), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Đường dẫn tới thư mục chứa dữ liệu huấn luyện\n",
    "train_data_dir = '../../../dataset/data/'\n",
    "\n",
    "# Load pre-trained FaceNet model\n",
    "faceNet_model = load_model('20240606-model.h5')\n",
    "\n",
    "# Load training data and extract embeddings\n",
    "faces, embeddings, labels = load_train_data_and_extract_embeddings(train_data_dir, faceNet_model)\n",
    "\n",
    "# Chuyển đổi nhãn sang dạng số\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Huấn luyện một bộ phân loại SVM trên dữ liệu nhúng\n",
    "svm_classifier = SVC(C=1.0, kernel='linear', probability=True)\n",
    "svm_classifier.fit(embeddings, labels_encoded)\n",
    "\n",
    "# Lưu trọng số của bộ phân loại và label encoder\n",
    "with open('svm_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_classifier, f)\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm nhận diện khuôn mặt từ camera và dự đoán\n",
    "def recognize_faces_from_camera(model, classifier, label_encoder):\n",
    "    # Khởi tạo video stream từ camera\n",
    "    vs = cv2.VideoCapture(0)\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    while True:\n",
    "        # Đọc frame từ video stream\n",
    "        ret, frame = vs.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize frame để tăng tốc độ xử lý\n",
    "        frame = cv2.resize(frame, (50, 50))\n",
    "\n",
    "        # Chuyển đổi frame sang ảnh grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Sử dụng bộ phát hiện khuôn mặt Cascade Classifier của OpenCV\n",
    "        face_cascade = cv2.CascadeClassifier('../utils/haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Phát hiện được khuôn mặt, cắt ảnh khuôn mặt và trích xuất embedding\n",
    "            face_image = frame[y:y+h, x:x+w]\n",
    "            face_embedding = extract_face_embedding(face_image, model)\n",
    "\n",
    "            # Dự đoán nhãn của khuôn mặt\n",
    "            prediction = classifier.predict([face_embedding])[0]\n",
    "            probability = classifier.predict_proba([face_embedding])[0]\n",
    "            name = label_encoder.inverse_transform([prediction])[0]\n",
    "            prob = np.max(probability)\n",
    "\n",
    "            # Vẽ hộp giới hạn và hiển thị kết quả dự đoán trên frame\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            text = f'{name}: {prob:.2f}'\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "        # Hiển thị frame\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "        # Thoát khỏi vòng lặp nếu nhấn phím 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Giải phóng video stream và đóng cửa sổ hiển thị\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVM classifier và label encoder từ file\n",
    "with open('svm_classifier.pkl', 'rb') as f:\n",
    "    loaded_classifier = pickle.load(f)\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    loaded_label_encoder = pickle.load(f)\n",
    "\n",
    "# Sử dụng mô hình nhận diện khuôn mặt và SVM classifier để nhận diện từ camera\n",
    "recognize_faces_from_camera(faceNet_model, loaded_classifier, loaded_label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
